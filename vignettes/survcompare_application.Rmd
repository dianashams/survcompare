---
title: "Survcompare examples"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{survcompare_application}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

## Survcompare package: background information.

The package checks whether there are considerable non-linear and interaction terms in the data, and quantifies their contributions to the models' performance. Using repeated nested cross-validation (that is, randomly splitting the data into the training and testing parts and evaluating test predictions), the package:

  * Validates Cox Proportionate Hazards model, or Cox Lasso depending on the user's input. This step employs `survival::coxph()` or `glmnet::glmnet(..., family="cox")` functions.

  * Validates Survival Random Forest ensembled with the baseline Cox model. To fit the ensemble, Cox-PH's out-of-sample predictions are added to the list of orignial predictors, which are then used to fit SRF, using `randomForestSRC::rfsrc()`.

  * Performs statistical testing of whether the Survival Random Forest ensemble outperforms the Cox model.
  
Predictive performance is evaluated by averaging different measures across all train-test splits. The following measures are computed:

 * Discrimination measures: Harrell's concordance index, time-dependent AUCROC.
 
 * Calibration measures: calibration slope, calibration alpha.
 
 * Overall fit: Brier score, Scaled Brier score. 

Performance metrics' description and definitions can be found, for example, here: 

Steyerberg, E. W., & Vergouwe, Y. (2014). Towards better clinical prediction models: seven steps for development and an ABCD for validation. _European heart journal_, 35(29), 1925-1931.

https://academic.oup.com/eurheartj/article/35/29/1925/2293109

### How do I use the results?

First, try to run sufficient number of repetitions (repeat_cv), at least 5, ideally 20-50 depending on the data heterogeneity and size. There are two possible outcomes: 1) "Survival Random Forest ensemble has NOT outperformed CoxPH", or 2) "Survival Random Forest ensemble has outperformed CoxPH by ... in C-index". 

  1) If there is *no outperformance*, the test can justify the employment of CoxPH model and indicate a negligible advantage of using a more flexible model such as Survival Random Forest.
  
  2) In the case of the ensemble's *outperformance* of the Cox model, a researcher can:
  
    - Employ a more complex or flexible model.
  
    - Look for the interaction and non-linear terms that could be added to the CoxPH and re-run the test. 
  
    - Consider using the CoxPH model despite its underperformance if the difference is not large enough to sacrifice model's interpretability, or negligible in the context of a performed task.

### Why the ensemble and not just SRF?

The ensemble of Cox and SRF takes the predictions of the Cox model and adds to the list of predictors to train SRF. This way, we make sure that linearity is captured by SRF at least as good as in the Cox model, and hence the marginal outperformance of the ensemble over the Cox model can be fully attributed to the qualities of SRF that Cox does not have, that is, data complexity.

## Package installation.

``` {r setup}
# install.packages("devtools")
devtools::install_github("dianashams/survcompare")
```

## Examples using simulated data

### Example 1. Linear data.

The first example takes simulated data that does not contain any complex terms, and Cox-PH is expected to perform as good as Survival Random Forest.

```{r example_simulated_1}

library(survcompare)
mydata <- simulate_linear()

mypredictors <- names(mydata)[1:4]

compare_models <- survcompare(mydata, mypredictors)

summary(compare_models)

```

### Example 2. Non-linear data with interaction terms.

The second example takes simulated data that contains non-linear and cross-terms, and an outperformance of the tree model is expected. We will increase the default number of cross-validation repetitions to get a robust estimate, and choose CoxLasso as our baseline linear model.

```{r example_simulated_2}

library(survcompare)

mydata2 <- simulate_crossterms()

mypredictors2 <- names(mydata)[1:4]

compare_models2 <- survcompare(mydata2, mypredictors2, outer_cv = 3, inner_cv = 2, repeat_cv = 10, useCoxLasso = TRUE)

summary(compare_models2)

```

Detailed results can be extracted from the output model. For example, test performance for each data split (across all cross-validations and repetitions). 

```{r}
compare_models2$test
```

### Example 3. Applying survcompare to GBSG data.

Now, lets apply the package to a real life data. We will use GBSG data from the survival package (https://rdrr.io/cran/survival/man/gbsg.html).

```{r example_3_gbsg}
library(survival)
mygbsg <- gbsg
mygbsg$time<- gbsg$rfstime/365
mygbsg$event<- gbsg$status
myfactors <- c("age", "meno", "size", "grade", "nodes", "pgr", "er", "hormon")
mygbsg<- mygbsg[c("time", "event", myfactors)]

sum(is.na(mygbsg[myfactors]))

survcompare(mygbsg, myfactors)

```

We got the following results (depending on random seed, it may slightly differ): 

"Survival Random Forest ensemble has outperformed CoxPH by 0.0134 in C-index. The difference is statistically significant with the p-value 0.00074325***. The supplied data may contain non-linear or cross-term dependencies, better captured by the Survival Random Forest. C-score: CoxPH 0.6762(95CI=0.6327-0.7151;SD=0.0272), SRF_Ensemble 0.6895(95CI=0.648-0.7293;SD=0.0254)."
  
This illustrates situation when outperformance of the non-linear model is statistically significant, but not large in absolute terms. The ultimate model choice will depend on how critical the 0.01 improvement in C-index is perceived by the model stakeholders.

