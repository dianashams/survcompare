---
title: "Model Validation and Cross Validation using survcompare"
author: "Diana Shamsutdinova"
date: "2023-11-13"
output:
  html_document: default
  pdf_document: default
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
devtools::load_all(".", reset = TRUE)
```

## Package background

The main application of the survcompare package it to check whether there are considerable non-linear and interaction terms in the data, and to quantify their contributions to the models' performance. Please refer to https://github.com/dianashams/survcompare/blob/main/README.md.

However, as the package uses Cox Proportionate Hazards and Survival Random Forest models, some of the developed functions may be helpful on its own, in particular, to perform a repeated nested cross-validation of these two underlying models.

## Other use cases of `survcompare`

### 1. Fitting, cross-validating, and predicting event probabilities using `survcompare` functionality for CoxPH and CoxLasso

```{r coxph_cv}
#devtools::install_github("dianashams/survcompare",build_vignettes = TRUE)
#library(survcompare)

# We will use the GBSG data from survival package. 
# Prepare the data for 'survcompare':
library(survival)
mygbsg <- gbsg
mygbsg$time<- gbsg$rfstime/365
mygbsg$event<- gbsg$status
myfactors <- c("age", "meno", "size", "grade", "nodes", "pgr", "er", "hormon")
mygbsg<- mygbsg[c("time", "event", myfactors)]
sum(is.na(mygbsg[myfactors])) # 0 -> no missing data,can use survcompare

# Make a random split into train (400) and test (the rest 286 observations)
train_index<- sample(seq(nrow(mygbsg)), 400, replace = FALSE)
traindf<- mygbsg[train_index, ]
testdf <- mygbsg[-train_index, ]

# define the time at which time-dependent AUC-ROC and Brier scores are to be evaluated
predict_time <- 6

# fit CoxPH 
cox1<- survcox_train(traindf, myfactors, useCoxLasso = FALSE)

# predict event probability at predict_time
cox_p6_test<- survcox_predict(cox1, testdf, predict_time)
cox_p6_train <- survcox_predict(cox1, traindf, predict_time)

# compute performance stats 
cox_performance_test <- 
  surv_validate(cox_p6_test, predict_time, traindf, testdf)
cox_performance_train <- 
  surv_validate(cox_p6_train, predict_time, traindf, traindf)
print(
  rbind("CoxTest" = cox_performance_test,
        "CoxTrain"= cox_performance_train))

# Further validation: repeated cross-validation 
coxcv <- survcox_cv(mygbsg, myfactors, predict_time)
summary(coxcv)

# Comments on the results: 
# wide confidence intervals show that the data is rather heterogeneous, 
# that is, quality of predictions differs substantially depending on 
# which random sub sample was chosen as the training data
# e.g. AUCROC can be anywhere from 0.62 to 0.92.
# 
# This illustrates that one should perform a repeated CV and acknowledge
# model variance rather than relying on a single train-test, where the
# results may be poor or excellent by chance and mask model's
# instability. 
```

### 2. Fitting, cross-validating, and predicting survival probabilities using `survcompare` functionality for Survival Random Forest.

Here, we perform the same for Survival Random Forest. First, let's train SRF on the train data:

```{r srf_cv}
library(survcompare)
# fit and tune SRF:
srfmodel1 <-
  survsrf_train(traindf,
                myfactors,
                predict_time,
                inner_cv = 2,
                randomseed = 42)

# predict event probability at predict_time
srf_p6_test<- survsrf_predict(srfmodel1, testdf, predict_time)
srf_p6_train <- survsrf_predict(srfmodel1, traindf, predict_time)

# compute performance stats 
srf_performance_test <- 
  surv_validate(srf_p6_test, predict_time, traindf, testdf)
srf_performance_train <- 
  surv_validate(srf_p6_train, predict_time, traindf, traindf)
print(
  rbind("SRFTest" = srf_performance_test,
        "SRFTrain"= srf_performance_train))

# Further validation: repeated cross-validation. We keep low repeat_cv number for quick compilation, but even 2 repetitions of the 3-fold CVs show large variance of the SRF predictions, similar to that of CoxPH.
srfcv <- survsrf_cv(mygbsg,
                    myfactors,
                    predict_time,
                    inner_cv = 2)

summary(srfcv)

```

##### Notes on the SRF tuning

Note, that the function `survsrf_train()` does not just fit a default SRF version, but uses a k-fold CV (k is controlled by `inner_cv`) to tune its hyperparameters mtry, nodesize, and nodedepth. To remind,

-   mtry - number of variables randomly selected as candidates for splitting a node
-   nodedepth - maximum depth to which a tree is grown.
-   nodesize - minumum size of terminal node.

There are more hyperparameters in the underlying `randomForestSRC::rfsrc()` function, however, these are the ones that tend to affect prediction accuracy the most. See also <https://CRAN.R-project.org/package=randomForestSRC>.

The tuning values for mtry, nodesize and nodedepth depend on the data size and total number of predictors. One can check the tuning grid and the final optimized parameters by running: 
```{r, srf_tuning_params}
# Tuning grid:
print(srfmodel1$grid)

# Selected hyperparameters and their performance metrics (measured on out-of-sample observations):
print(srfmodel1$bestparams)  
```

It is possible to use a custom grid to tune SRF. First, create a list containing three numerical vectors, named as "mtry", "nodesize", and "nodedepth", containing their candidate values. Then, supply the list as an argument to `survsrf_train()`. We suggest running default option, and then adjusting the grid to include higher or lower values, if the highest or lowest value was chosen in the default run. 

```{r srf_custom_tuning, eval=FALSE}
# creating list for the customised tuning of SRF:
my_srf_tuning <-
  list(
    "mtry" = c(2),
    "nodesize" = c(10, 20, 30),
    "nodedepth" = c(15, 25, 35)
  )
srfmodel2 <-
  survsrf_train(
    traindf,
    myfactors,
    predict_time,
    tuningparams = my_srf_tuning,
    inner_cv = 2,
    randomseed = 42
  )

# Compare the fits (we see some marginal improvement)
rbind(
  default_tune = srfmodel1$bestparams,
  custom_tune = srfmodel2$bestparams )

#          mtry nodesize nodedepth time AUCROC  BS   BS_scaled C_score
#default_tune	 2	 20	    25	     6	   0.730	0.179	0.273	    0.673	
#custom_tune	 2	 10	    25	     6	   0.769	0.173	0.300   	0.668

```


